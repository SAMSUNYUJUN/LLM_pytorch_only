"""
åˆ†å¸ƒå¼AdamWï¼Œç±»ä¼¼ZeRO-2ï¼Œå³åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦å½’çº¦ã€‚

AdamWä¼˜åŒ–å™¨è§£é‡Šï¼š
pytorchä¸­AdamWçš„è¶…å‚æ•°ï¼š
torch.optim.AdamW(
    params,
    lr=1e-3,
    betas=(0.9, 0.999),
    eps=1e-8,
    weight_decay=0.01,
)
-lrï¼ˆå­¦ä¹ ç‡ Î±ï¼‰ï¼šå†³å®šæ•´ä½“æ­¥å­å¤§å°ï¼Œè¶Šå¤§èµ°å¾—è¶Šå¿«ï¼Œä¹Ÿè¶Šå®¹æ˜“å‘æ•£ã€‚Adam ç³»åˆ—é€šå¸¸ 1e-3, 3e-4, 1e-4 è¿™ä¸€æ¡£ã€‚
-betas = (Î²1, Î²2)ï¼šÎ²1ï¼šä¸€é˜¶åŠ¨é‡çš„è¡°å‡ç³»æ•°ï¼ˆå¯¹æ¢¯åº¦æœ¬èº«åšæ»‘åŠ¨å¹³å‡ï¼‰ä¸€èˆ¬ 0.9 å·¦å³ â†’ è¡¨ç¤ºâ€œæ–°çš„æ¢¯åº¦å  10%ï¼Œæ—§çš„è®°å¿†å  90%â€ï¼› Î²2ï¼šäºŒé˜¶åŠ¨é‡çš„è¡°å‡ç³»æ•°ï¼ˆå¯¹æ¢¯åº¦å¹³æ–¹åšæ»‘åŠ¨å¹³å‡ï¼‰ä¸€èˆ¬ 0.999 / 0.98 â†’ å¹³æ»‘æ¯ä¸€ç»´æ¢¯åº¦çš„æ–¹å·®ã€‚
-epsï¼ˆÎµï¼‰é˜²æ­¢åˆ†æ¯ä¸º 0 çš„å°å¸¸æ•°ï¼Œä¸€èˆ¬ 1e-8ã€‚æ•°å€¼éå¸¸å°ï¼Œåªåœ¨æ¢¯åº¦æå°æˆ–åˆšå¼€å§‹æ—¶èµ·ä¸€ç‚¹ç¨³å®šä½œç”¨ã€‚
- weight_decayï¼ˆæƒé‡è¡°å‡ Î»ï¼‰å°±æ˜¯ L2 æ­£åˆ™å¼ºåº¦ã€‚å…¸å‹èŒƒå›´ï¼š0 ~ 0.1ï¼Œå¤§æ¨¡å‹é‡Œå¸¸è§ 0.01 å·¦å³ã€‚

ä¸€é˜¶åŠ¨é‡ï¼š mtâ€‹=Î²1 * â€‹mtâˆ’1 â€‹+(1âˆ’Î²1â€‹)gtâ€‹
mtåˆå§‹å€¼ä¸º0ã€‚mtæ˜¯æ»‘åŠ¨å¹³å‡ã€‚ç›´è§‚ç±»æ¯”ï¼šä½ åœ¨æ¨ä¸€ä¸ªçƒä¸‹å¡ï¼šå½“å‰æ¢¯åº¦ g_t = å½“å‰â€œå¡çš„æ–¹å‘â€ï¼Œm_t = çƒçš„é€Ÿåº¦ï¼ˆæœ‰æƒ¯æ€§ï¼Œä¸ä¼šç«‹åˆ»æ‰å¤´ï¼‰ï¼ŒÎ²1 è¶Šå¤§ï¼Œæƒ¯æ€§è¶Šå¼ºï¼Œè¶Šâ€œæ»‘â€ã€‚æ¥ä¸€ä¸ª 1D å…·ä½“å°ä¾‹å­ï¼š

Î²1 = 0.9ï¼› ğ‘š0=0ï¼› ä¸‰ä¸ª step çš„æ¢¯åº¦ï¼šgâ‚ = 1ï¼› gâ‚‚ = 1 ï¼›gâ‚ƒ = -1ï¼ˆæ–¹å‘çªç„¶åäº†ï¼‰
æˆ‘ä»¬ç®—ä¸€ä¸‹ï¼š
t = 1ï¼šm1=0.9â‹…0+0.1â‹…1=0.1
t = 2ï¼šm2 =0.9â‹…0.1+0.1â‹…1=0.09+0.1=0.19
t = 3ï¼ˆæ¢¯åº¦å˜æˆ -1ï¼‰ï¼šm3=0.9â‹…0.19+0.1â‹…(-1)=0.171-0.1=0.071
å•çœ‹æ¢¯åº¦ï¼š[1, 1, -1] â†’ ç¬¬ä¸‰æ­¥çªç„¶åå‘, åŠ¨é‡ mï¼š[0.1, 0.19, 0.071] â†’ åªæ˜¯æ…¢æ…¢å¾€ä¸‹å‡ï¼Œè¿˜æ²¡æœ‰ç«‹åˆ»å˜æˆè´Ÿæ•°, è¿™å°±è¯´æ˜ï¼šåŠ¨é‡ä¸æ˜¯â€œå½“å‰è¿™ä¸€æ­¥çš„æ¢¯åº¦â€ï¼Œæ˜¯â€œè¿™æ®µæ—¶é—´æ¢¯åº¦çš„å¹³å‡è¶‹åŠ¿â€ï¼Œå¸®ä½ å¹³æ»‘éœ‡è¡ + æé«˜æ”¶æ•›æ•ˆç‡ã€‚

åœ¨ AdamW ä¸­ï¼ŒçœŸæ­£æ›´æ–°å‚æ•°çš„ä¸æ˜¯ g_tï¼Œè€Œæ˜¯ bias-correct åçš„ mÌ‚_t = m_t / (1 - Î²1^t)ï¼Œå› ä¸º m_t åˆšå¼€å§‹æ—¶åå‘äº 0ï¼Œéœ€è¦æ ¡æ­£ã€‚éšç€ t å˜å¤§ï¼Œ(1 - Î²1^t) è¶‹è¿‘äº 1ï¼Œæ ¡æ­£ä½œç”¨å‡å°ã€‚
	â€‹
äºŒé˜¶åŠ¨é‡ï¼š vtâ€‹=Î²2 * â€‹vtâˆ’1 â€‹+(1âˆ’Î²2â€‹)gtâ€‹^2
vt åˆå§‹å€¼ä¸º 0ã€‚vt ä¹Ÿæ˜¯æ»‘åŠ¨å¹³å‡ï¼Œä½†å®ƒæ˜¯å¯¹æ¢¯åº¦å¹³æ–¹çš„æ»‘åŠ¨å¹³å‡ï¼Œè¡¨ç¤ºâ€œæ¯ä¸€ç»´æ¢¯åº¦çš„æ–¹å·®â€ã€‚ç±»æ¯”ä¸Šé¢çš„ä¾‹å­ï¼š
Î²2 = 0.999ï¼› ğ‘£0=0ï¼› ä¸‰ä¸ª step çš„æ¢¯åº¦ï¼šgâ‚ = 1ï¼› gâ‚‚ = 1 ï¼›gâ‚ƒ = -1
t = 1ï¼šv1=0.999â‹…0+0.001â‹…1Â²=0.001
t = 2ï¼šv2=0.999â‹…0.001+0.001â‹…1Â²=0.000999+0.001=0.001999
t = 3ï¼šv3=0.999â‹…0.001999+0.001â‹…(-1)Â²=0.001997001
å¯ä»¥çœ‹åˆ°ï¼Œv_t å˜åŒ–éå¸¸æ…¢ï¼Œè¡¨ç¤ºâ€œè¿™å‡ æ­¥æ¢¯åº¦çš„æ–¹å·®å¤§æ¦‚åœ¨ 0.002 å·¦å³â€ã€‚ä¸ºä»€ä¹ˆè¦å…³æ³¨æ–¹å·®ï¼Ÿå› ä¸ºï¼šå¦‚æœæŸä¸€ç»´æ¢¯åº¦æ–¹å·®å¾ˆå¤§ï¼Œè¯´æ˜è¿™ä¸€ç»´æ¢¯åº¦ä¸ç¨³å®šï¼Œå¯èƒ½å¿½å¤§å¿½å°ï¼Œç›´æ¥ç”¨å®ƒæ¥æ›´æ–°å‚æ•°ä¼šä¸ç¨³ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦â€œæŠ‘åˆ¶â€å®ƒçš„æ›´æ–°ã€‚
åœ¨ AdamW ä¸­ï¼ŒçœŸæ­£ç”¨æ¥æ›´æ–°å‚æ•°çš„ä¸æ˜¯ v_tï¼Œè€Œæ˜¯ bias-correct åçš„ vÌ‚_t = v_t / (1 - Î²2^t)ï¼ŒåŒæ ·æ˜¯ä¸ºäº†æ ¡æ­£åˆšå¼€å§‹æ—¶çš„åå·®ã€‚

å‚æ•°æ›´æ–°å…¬å¼ï¼š
p - lr * m_hat_t / (sqrt(v_hat_t) + eps)
æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œè¿™ä¸ªè¿‘ä¼¼äºåœ¨ç»™lråšè°ƒæ•´ï¼Œè°ƒæ•´çš„æ˜¯å‚æ•°çš„å¹³å‡å€¼å’Œæ–¹å·®ï¼šæˆ‘ä»¬ç”¨åŠ¨é‡çš„æ–¹å¼æ¥ä¼°è®¡æ¢¯åº¦çš„å‡å€¼ mÌ‚_t å’Œæ–¹å·® vÌ‚_tï¼Œè¿™æ ·æ¯”ç›´æ¥ç®—æ‰€æœ‰å†å²æ¢¯åº¦çš„å‡å€¼å’Œæ–¹å·®è¦é«˜æ•ˆå¾—å¤šã€‚è€Œä¸”å¯ä»¥è®©å½“å‰å½“å‰çš„å¹³å‡å’Œæ–¹å·®æ›´å…³æ³¨è¿‘æœŸçš„æ¢¯åº¦å˜åŒ–ï¼Œæ›´çµæ´»åœ°é€‚åº”å½“å‰çš„ä¼˜åŒ–å½¢åŠ¿ã€‚
"""
import torch
import torch.distributed as dist
from torch import Tensor


class DistAdamW(torch.optim.Optimizer):
    """
    Distributed AdamW optimizer.
    In the style of ZeRO-2, i.e. sharded optimizer states and gradient reduction
    """

    # param_groupsæ˜¯çŸ©é˜µå‚æ•°å’Œå­¦ä¹ ç‡[{"params": [W_q, W_k, ...], "lr": 0.02, ...},  {"params": [embedding], "lr": 0.2, ...},...]
    def __init__(self, param_groups, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        # rankæŒ‡è¿›ç¨‹ï¼Œä¸€èˆ¬æ¥è¯´ä¸€ä¸ªrankå¯¹åº”ä¸€å¼ gpu
        # rankä¼šåŠ¨æ€æ£€æµ‹è¿”å›å½“å‰è¿›ç¨‹çš„rank idï¼Œ å¦‚æœåœ¨ç¬¬ä¸€å¼ å¡ä¸Šï¼Œrankå°±æ˜¯0ï¼Œç¬¬äºŒå¼ å¡ä¸Šrankå°±æ˜¯1ã€‚
        rank = dist.get_rank()
        # è¿›ç¨‹æ€»æ•°
        world_size = dist.get_world_size()
        #å‡†å¤‡ä¸‰ä¸ªåˆ—è¡¨ï¼š
        #reduce_scatter_futuresï¼šå­˜æ¯ä¸ª reduce_scatter çš„ futureï¼Œåé¢è¦ .wait()ï¼›
        #all_reduce_futuresï¼šå­˜æ¯ä¸ª all_gather çš„ futureï¼›
        #grad_slicesï¼šå­˜æ¯ä¸ªå‚æ•°å¯¹åº”çš„ æ¢¯åº¦åˆ‡ç‰‡ï¼ˆæ¯ä¸ª rank æ‹¿è‡ªå·±é‚£ä¸€ç‰‡ï¼‰ã€‚
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []

        # ç¬¬ä¸€è½®å¾ªç¯ï¼Œå¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦åš reduce_scatter æ“ä½œï¼ŒæŠŠç»“æœæ”¾åˆ° grad_slices é‡Œ
        # reduce_scatter æ˜¯æŠŠä¸€ä¸ªå¼ é‡æŒ‰ç¬¬0ç»´åˆ‡åˆ†æˆ world_size ä»½ï¼Œç„¶åæ¯ä¸ª rank æ‹¿å…¶ä¸­ä¸€ä»½ï¼Œå¹¶å¯¹æ‰€æœ‰ rank çš„å¯¹åº”ä»½åš reduce æ“ä½œï¼ˆè¿™é‡Œæ˜¯æ±‚å’Œå†å¹³å‡ï¼‰
        # ä¾¿åˆ©param_groupsé‡Œçš„æ¯ä¸ªå‚æ•°ç»„
        # è¿™é‡Œæœ‰ä¸€ä¸ªå¼‚æ­¥è®¡ç®—ï¼Œå¯ä»¥å…ˆå¾ªç¯æŠŠæ‰€æœ‰çš„ reduce_scatter_tensor æ“ä½œéƒ½å‘èµ·ï¼Œç„¶åå†ç­‰å®ƒä»¬å®Œæˆ
        for group in self.param_groups:
            # å¾—åˆ°è¯¥ç»„çš„å‚æ•°åˆ—è¡¨
            params: list[Tensor] = group["params"]
            # ä¾¿åˆ©è¯¥ç»„çš„æ¯ä¸ªå‚æ•°
            for base_i in range(len(params)):
                # å¾—åˆ°è¯¥å‚æ•°çš„æ¢¯åº¦
                grad = params[base_i].grad
                # æŒ‰ rank æ•°é‡åˆ‡åˆ†æ¢¯åº¦
                rank_size = grad.shape[0] // world_size
                # å‡†å¤‡ä¸€ä¸ªç©ºå¼ é‡çš„å½¢çŠ¶æ˜¯æ¢¯åº¦åˆ†æˆä¸¤åŠåçš„å½¢çŠ¶ ï¼ˆä¸¤å¼ gpuï¼‰
                grad_slice = torch.empty_like(grad[:rank_size])
                # dist.reduce_scatter_tensor æ˜¯ä¸ªå…¨å±€é€šä¿¡æ“ä½œï¼Œç›¸å½“äºä¸€ä¸ªåˆ†é…å‘˜ã€‚
                # å®ƒä¼šæŠŠæ‰€æœ‰ rank çš„ grad å…ˆæŒ‰å…ƒç´  AVGï¼›å†æŠŠ avg_grad åˆ†æˆä¸¤æ®µï¼šå‰ä¸€åŠè¡Œåˆ†é…ç»™ rank 0ï¼Œåä¸€åŠè¡Œåˆ†é…ç»™ rank 1ï¼›è®©ä»–ä»¬åˆ†åˆ«è¿›è¡Œè®¡ç®—ã€‚
                # ç„¶ååˆ†åˆ«å­˜åœ¨å„è‡ªrankçš„ grad_slice é‡Œï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯å¼ å¡ä¸Šæœ‰è‡ªå·±å•ç‹¬çš„grad_slices.
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                # æŠŠè¯¥å‚æ•°å¯¹åº”çš„æ¢¯åº¦åˆ‡ç‰‡å­˜èµ·æ¥
                grad_slices.append(grad_slice)

        # ç¬¬äºŒè½®åŒå¾ªç¯ï¼šç­‰å¾…æ¢¯åº¦é€šä¿¡å®Œæˆ â†’ å¯¹æ¯ç‰‡å‚æ•°åš AdamW æ›´æ–° â†’ all_gather æ‹¼å›å‚æ•°
        idx = 0
        # ä¾æ—§éå†æ¯ä¸ªå‚æ•°ç»„
        for group in self.param_groups:
            # å–å‡º AdamW çš„è¶…å‚
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            # å–å‡ºè¯¥ç»„çš„å‚æ•°
            params = group['params']

            # éå†è¯¥ç»„çš„æ¯ä¸ªå‚æ•°
            for base in range(len(params)):
                # ç­‰å¾…å¯¹åº”çš„ reduce_scatter æ“ä½œå®Œæˆï¼Œæ‹¿åˆ°è¯¥ rank çš„æ¢¯åº¦åˆ‡ç‰‡
                reduce_scatter_futures[idx].wait()
                # åˆ‡å‡ºå½“å‰ rank è‡ªå·±è´Ÿè´£çš„å‚æ•°å— p_sliceï¼Œå¹¶åªæ›´æ–°è¿™éƒ¨åˆ†å‚æ•°
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]

                # å–å­¦ä¹ ç‡ã€çŠ¶æ€ã€æ¢¯åº¦åˆ‡ç‰‡
                # lrï¼šè¿™ä¸€ç»„å‚æ•°çš„å­¦ä¹ ç‡ï¼Œå¦‚æœ p ä¸ŠæŒ‚äº† p.lr_mulï¼Œå†ä¹˜ä¸€ä¸ªç¼©æ”¾ï¼ˆæ¯”å¦‚æœ‰çš„å±‚ç”¨æ›´å° lrï¼‰
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                # state = self.state[p]ï¼šè¿™æ˜¯æ¯ä¸ªå‚æ•°ä¸€ä¸ªå­—å…¸ï¼Œç”¨æ¥å­˜ï¼šstepï¼ˆç¬¬å‡ æ­¥ï¼‰ï¼›exp_avgï¼ˆä¸€é˜¶åŠ¨é‡ï¼‰ï¼›exp_avg_sqï¼ˆäºŒé˜¶åŠ¨é‡ï¼‰
                state = self.state[p]
                # å–å‡ºå¯¹åº”çš„æ¢¯åº¦åˆ‡ç‰‡
                g_slice = grad_slices[idx]

                # State init
                # ç¬¬ä¸€æ¬¡ step æ—¶ï¼Œstate æ˜¯ç©ºçš„ï¼Œèµ°åˆå§‹åŒ–ï¼š
                # step = 0
                # exp_avgã€exp_avg_sq çš„ shape å’Œ p_slice ä¸€æ ·ï¼š
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                # å–å‡ºå½“å‰stepçš„ä¸€é˜¶ã€äºŒé˜¶åŠ¨é‡
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                # æ›´æ–°step
                state['step'] += 1
                t = state['step']

                # weight decay
                # åœ¨p_sliceä¸Šåšweightsè¡°å‡
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)

                # update running averages
                # æ›´æ–°ä¸€é˜¶ã€äºŒé˜¶åŠ¨é‡
                # ä¸€é˜¶åŠ¨é‡ mtâ€‹=Î²1 * â€‹mtâˆ’1 â€‹+(1âˆ’Î²1â€‹)gtâ€‹
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                # äºŒé˜¶åŠ¨é‡ vtâ€‹=Î²2 * â€‹vtâˆ’1 â€‹+(1âˆ’Î²2â€‹)gtâ€‹^2
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # ---- bias correction åˆ†æ¯ ----
                # m_hat_tâ€‹=mtâ€‹/(1âˆ’Î²1 ** tâ€‹)ï¼Œ è¿™é‡Œåªè®¡ç®—äº†åˆ†æ¯éƒ¨åˆ†
                bias1 = 1 - beta1 ** t
                # v_hat_tâ€‹=vtâ€‹/(1âˆ’Î²2 ** tâ€‹)ï¼Œ è¿™é‡Œåªè®¡ç®—äº†åˆ†æ¯éƒ¨åˆ†
                bias2 = 1 - beta2 ** t

                #   p â† p - lr * m_hat_t / (sqrt(v_hat_t) + eps)
                #   = p - lr * [m_t / (1 - beta1^t)] / (sqrt(v_t / (1 - beta2^t)) + eps)
                m_hat = exp_avg / bias1           # mÌ‚_t
                v_hat = exp_avg_sq / bias2        # vÌ‚_t
                denom = v_hat.sqrt().add_(eps)    # sqrt(vÌ‚_t) + eps
                step_size = lr                   # lr
                update = m_hat / denom * step_size  # lr * mÌ‚_t / (sqrt(vÌ‚_t) + eps)

                # æ›´æ–°å‚æ•° p â† p - lr * m_hat_t / (sqrt(v_hat_t) + eps)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1

                # all gather updated param slice back to full param
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # ç­‰å¾…æ‰€æœ‰ all_gather æ“ä½œå®Œæˆ
        torch.futures.collect_all(all_reduce_futures).wait()
